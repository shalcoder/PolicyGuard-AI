from fastapi import APIRouter, UploadFile, File, HTTPException
from models.policy import PolicyDocument, WorkflowDefinition, ComplianceReport
from models.settings import PolicySettings
from services.ingest import PolicyIngestor
from services.gemini import GeminiService
from services.storage import policy_db
import json

router = APIRouter()
ingestor = PolicyIngestor()
gemini = GeminiService()

@router.post("/policies/upload", response_model=PolicyDocument)
async def upload_policy(file: UploadFile = File(...)):
    if not file.filename.endswith(('.txt', '.pdf', '.docx', '.md', '.json')):
        raise HTTPException(status_code=400, detail="Invalid file type")
    
    content = await file.read()
    # Basic decoding
    text_content = content.decode('utf-8', errors='ignore')
    
    # Ingest
    cleaned_text = await ingestor.ingest_text(file.filename, text_content)

    # Analyze with Gemini (Immediate Feedback)
    try:
        print(f"Summarizing policy: {file.filename}")
        summary = await gemini.summarize_policy(cleaned_text)
    except Exception as e:
        print(f"Gemini Error during summary: {e}")
        # import traceback
        # traceback.print_exc()
        summary = "Summary unavailable (AI Error)"
    
    import uuid
    policy_id = str(uuid.uuid4())
    
    policy = PolicyDocument(
        id=policy_id,
        name=file.filename,
        content=cleaned_text,
        summary=summary
    )
    
    # Save to In-Memory DB
    policy_db.add_policy(policy)
    
    return policy

@router.get("/policies", response_model=list[PolicyDocument])
async def get_policies():
    return policy_db.get_all_policies()

@router.delete("/policies/{policy_id}")
async def delete_policy(policy_id: str):
    success = policy_db.delete_policy(policy_id)
    if not success:
        raise HTTPException(status_code=404, detail="Policy not found")
    return {"status": "deleted", "id": policy_id}

@router.patch("/policies/{policy_id}/toggle")
async def toggle_policy(policy_id: str):
    # Find current status
    policies = policy_db.get_all_policies()
    target = next((p for p in policies if p.id == policy_id), None)
    
    if not target:
        raise HTTPException(status_code=404, detail="Policy not found")
    
    new_status = not target.is_active
    updated_policy = policy_db.update_policy(policy_id, {"is_active": new_status})
    
    return updated_policy

@router.post("/evaluate", response_model=ComplianceReport)
async def evaluate_workflow(workflow: WorkflowDefinition):
    try:
        print(f"Evaluating workflow: {workflow.name}")
        # Retrieve active policies
        # Retrieve active policies
        all_policies = policy_db.get_all_policies()
        policies = [p for p in all_policies if p.is_active]
        print(f"Active policies count: {len(policies)}")
        
        if not policies:
            print("No policies found returning Error")
            # If no policies, cannot generate report
            raise HTTPException(status_code=400, detail="No active policies found. Please upload a policy first.")

        # context construction (Concatenate all policies)
        policy_context = "\n\n".join([f"Policy '{p.name}':\n{p.content}" for p in policies])
        
        # Real Gemini Analysis
        print("Calling Gemini...")
        print(f"Workflow Desc: {workflow.description[:100]}...")
        
        # Fetch current settings to perform dynamic analysis
        settings = policy_db.get_settings()
        
        try:
            analysis_json_str = await gemini.analyze_policy_conflict(policy_context, workflow.description, settings)
        except Exception as e:
            print(f"Gemini API Error: {e}")
            raise HTTPException(status_code=503, detail=f"AI Service Unavailable: {str(e)}")
            
        print(f"Gemini Response Length: {len(analysis_json_str)}")
        
        # Clean JSON string (remove markdown fences if present - though handled by SDK mostly)
        # Robust cleaning using regex
        import re
        # Find the first JSON object or array
        match = re.search(r'(\{.*\}|\[.*\])', analysis_json_str, re.DOTALL)
        if match:
            clean_json = match.group(1)
        else:
            clean_json = analysis_json_str.strip()
            
        print(f"Cleaned JSON Preview: {clean_json[:100]}...")
        
        try:
            result_data = json.loads(clean_json)
            # Direct Pydantic validation
            report = ComplianceReport(**result_data)
            
            # Save Report to History
            policy_db.add_evaluation(report.model_dump())
            
            return report
            
        except json.JSONDecodeError as e:
            print(f"JSON Decode Error: {e}")
            print(f"Bad JSON: {analysis_json_str}")
            raise HTTPException(status_code=500, detail="AI returned invalid JSON format")
        except Exception as e:
            print(f"Validation Error: {e}")
            raise HTTPException(status_code=500, detail=f"Report Validation Failed: {str(e)}")

    except Exception as e:
        print(f"Server Error during evaluation: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/dashboard/stats")
async def get_dashboard_stats():
    return policy_db.get_dashboard_stats()

from services.sla_predictor import sla_predictor
from pydantic import BaseModel

class TelemetryData(BaseModel):
    service_id: str
    error_rate: float
    latency_ms: float
    request_count: int

@router.post("/telemetry/ingest")
async def ingest_telemetry(data: TelemetryData):
    sla_predictor.ingest_metrics(data.service_id, data.model_dump())
    return {"status": "ingested"}

@router.get("/telemetry/risk/{service_id}")
async def get_risk_prediction(service_id: str):
    return sla_predictor.predict_risk(service_id)

@router.get("/telemetry/history/{service_id}")
async def get_risk_history(service_id: str):
    return sla_predictor.get_risk_history(service_id)

@router.get("/dashboard/monitor")
async def get_monitor_data():
    return policy_db.get_monitor_data()

@router.get("/settings", response_model=PolicySettings)
async def get_settings():
    return policy_db.get_settings()

@router.post("/settings", response_model=PolicySettings)
async def update_settings(settings: PolicySettings):
    policy_db.save_settings(settings)
    return settings

@router.post("/simulate")
async def run_simulation():
    # In a real scenario, this would trigger a test eval against a known "bad" input
    # For the MVP, we mock the delay and return the structured result for the frontend
    import asyncio
    await asyncio.sleep(2) # Simulate processing time
    
    return {
        "status": "completed",
        "risks_found": 2,
        "details": [
            "Data Privacy Violation: PII detected in prompt template without encryption flag.",
            "Region Mismatch: Accessing EU user data from US server region."
        ]
    }

class SLAMetricsInput(BaseModel):
    latency_ms: float
    error_rate_percent: float
    uptime_percent: float
    support_response_time_hours: float
    service_name: str = "My AI Service"

@router.post("/sla/analyze")
async def analyze_sla_metrics(metrics: SLAMetricsInput):
    try:
        # Call Gemini
        analysis = await gemini.analyze_sla(metrics.model_dump())
        
        # Clean JSON
        import re
        match = re.search(r'(\{.*\}|\[.*\])', analysis, re.DOTALL)
        if match:
            clean_json = match.group(1)
        else:
            clean_json = analysis.strip()
            
        result = json.loads(clean_json)
        return result
    except Exception as e:
        print(f"SLA Analysis Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

